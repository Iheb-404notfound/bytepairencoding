{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4aa32e0-54e4-432f-8356-b2cdc2d045d2",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8e460967-a113-4191-830a-d3974d13f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, vocabs):\n",
    "    chars = list(text)\n",
    "    tokenized = chars[:1]\n",
    "    for char in chars[1:]:\n",
    "        print(tokenized[-1], char, tokenized[-1] + char in vocabs)\n",
    "        if tokenized[-1] + char in vocabs:\n",
    "            tokenized[-1] = tokenized[-1] + char\n",
    "        else:\n",
    "            tokenized.append(char)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e8ceae9a-dbac-4604-b3fc-f86ba89af1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(li, length=2, step=1):\n",
    "    i, j = 0, length\n",
    "    while j<=len(li):\n",
    "        yield li[i:j]\n",
    "        i+= step\n",
    "        j+= step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8dab1a93-e714-4c70-91dd-4f7c2253567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occs(text, vocabs):\n",
    "    itext = tokenize(text, vocabs)\n",
    "    occs = {}\n",
    "    for token1, token2 in sliding_window(itext):\n",
    "        if (token1,token2) in occs:\n",
    "            occs[(token1,token2)]+=1\n",
    "        else:\n",
    "            occs[(token1,token2)]=1\n",
    "    return occs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2c6de00b-4b0a-45e5-9fb8-0408ff10546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocabs(text, oldvocabs):\n",
    "    vocabs = oldvocabs.copy()\n",
    "    occs = count_occs(corpus, vocabs)\n",
    "    keys = max(occs.keys(), key=lambda x: occs[x])\n",
    "    vocabs.append(\"\".join(keys))\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "24f828ff-e1c0-4954-b7a6-8e9850ea51a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " t False\n",
      "t h True\n",
      "th i False\n",
      "i s False\n",
      "s   True\n",
      "s  i False\n",
      "i s False\n",
      "s   True\n",
      "s  t False\n",
      "t h True\n",
      "th e False\n",
      "e   False\n",
      "  h False\n",
      "h u False\n",
      "u g False\n",
      "g g False\n",
      "g i False\n",
      "i n False\n",
      "n g False\n",
      "g   False\n",
      "  f False\n",
      "f a False\n",
      "a c False\n",
      "c e False\n",
      "e   False\n",
      "  c False\n",
      "c o False\n",
      "o u False\n",
      "u r False\n",
      "r s False\n",
      "s e False\n",
      "e , False\n",
      ",   False\n",
      "  t False\n",
      "t h True\n",
      "th i False\n",
      "i s False\n",
      "s   True\n",
      "s  c False\n",
      "c h False\n",
      "h a False\n",
      "a p False\n",
      "p t False\n",
      "t e False\n",
      "e r False\n",
      "r   False\n",
      "  i False\n",
      "i s False\n",
      "s   True\n",
      "s  a False\n",
      "a b False\n",
      "b o False\n",
      "o u False\n",
      "u t False\n",
      "t   False\n",
      "  t False\n",
      "t o False\n",
      "o k False\n",
      "k e False\n",
      "e n False\n",
      "n i False\n",
      "i z False\n",
      "z a False\n",
      "a t False\n",
      "t i False\n",
      "i o False\n",
      "o n False\n",
      "n . False\n",
      ".   False\n",
      "  t False\n",
      "t h True\n",
      "th i False\n",
      "i s False\n",
      "s   True\n",
      "s  s False\n",
      "s e False\n",
      "e c False\n",
      "c t False\n",
      "t i False\n",
      "i o False\n",
      "o n False\n",
      "n   False\n",
      "  s False\n",
      "s h False\n",
      "h o False\n",
      "o w False\n",
      "w s False\n",
      "s   True\n",
      "s  s False\n",
      "s e False\n",
      "e v False\n",
      "v e False\n",
      "e r False\n",
      "r a False\n",
      "a l False\n",
      "l   False\n",
      "  t False\n",
      "t o False\n",
      "o k False\n",
      "k e False\n",
      "e n False\n",
      "n i False\n",
      "i z False\n",
      "z e False\n",
      "e r False\n",
      "r   False\n",
      "  a False\n",
      "a l False\n",
      "l g False\n",
      "g o False\n",
      "o r False\n",
      "r i False\n",
      "i t False\n",
      "t h True\n",
      "th m False\n",
      "m s False\n",
      "s . False\n",
      ". \n",
      " False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'th',\n",
       " 'i',\n",
       " 's ',\n",
       " 'i',\n",
       " 's ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'h',\n",
       " 'u',\n",
       " 'g',\n",
       " 'g',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'f',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'u',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " ',',\n",
       " ' ',\n",
       " 'th',\n",
       " 'i',\n",
       " 's ',\n",
       " 'c',\n",
       " 'h',\n",
       " 'a',\n",
       " 'p',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'i',\n",
       " 's ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'o',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " '.',\n",
       " ' ',\n",
       " 'th',\n",
       " 'i',\n",
       " 's ',\n",
       " 's',\n",
       " 'e',\n",
       " 'c',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 's',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " 's ',\n",
       " 's',\n",
       " 'e',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'a',\n",
       " 'l',\n",
       " 'g',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 'th',\n",
       " 'm',\n",
       " 's',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icorpus = tokenize(corpus, vocabs)\n",
    "icorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "962f6097-c9c5-4019-a579-1b2360605b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 's ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('\\n', 'th'): 1,\n",
       " ('th', 'i'): 3,\n",
       " ('i', 's '): 5,\n",
       " ('s ', 'i'): 1,\n",
       " ('s ', 'th'): 1,\n",
       " ('th', 'e'): 1,\n",
       " ('e', ' '): 2,\n",
       " (' ', 'h'): 1,\n",
       " ('h', 'u'): 1,\n",
       " ('u', 'g'): 1,\n",
       " ('g', 'g'): 1,\n",
       " ('g', 'i'): 1,\n",
       " ('i', 'n'): 1,\n",
       " ('n', 'g'): 1,\n",
       " ('g', ' '): 1,\n",
       " (' ', 'f'): 1,\n",
       " ('f', 'a'): 1,\n",
       " ('a', 'c'): 1,\n",
       " ('c', 'e'): 1,\n",
       " (' ', 'c'): 1,\n",
       " ('c', 'o'): 1,\n",
       " ('o', 'u'): 2,\n",
       " ('u', 'r'): 1,\n",
       " ('r', 's'): 1,\n",
       " ('s', 'e'): 3,\n",
       " ('e', ','): 1,\n",
       " (',', ' '): 1,\n",
       " (' ', 'th'): 2,\n",
       " ('s ', 'c'): 1,\n",
       " ('c', 'h'): 1,\n",
       " ('h', 'a'): 1,\n",
       " ('a', 'p'): 1,\n",
       " ('p', 't'): 1,\n",
       " ('t', 'e'): 1,\n",
       " ('e', 'r'): 3,\n",
       " ('r', ' '): 2,\n",
       " (' ', 'i'): 1,\n",
       " ('s ', 'a'): 1,\n",
       " ('a', 'b'): 1,\n",
       " ('b', 'o'): 1,\n",
       " ('u', 't'): 1,\n",
       " ('t', ' '): 1,\n",
       " (' ', 't'): 2,\n",
       " ('t', 'o'): 2,\n",
       " ('o', 'k'): 2,\n",
       " ('k', 'e'): 2,\n",
       " ('e', 'n'): 2,\n",
       " ('n', 'i'): 2,\n",
       " ('i', 'z'): 2,\n",
       " ('z', 'a'): 1,\n",
       " ('a', 't'): 1,\n",
       " ('t', 'i'): 2,\n",
       " ('i', 'o'): 2,\n",
       " ('o', 'n'): 2,\n",
       " ('n', '.'): 1,\n",
       " ('.', ' '): 1,\n",
       " ('s ', 's'): 2,\n",
       " ('e', 'c'): 1,\n",
       " ('c', 't'): 1,\n",
       " ('n', ' '): 1,\n",
       " (' ', 's'): 1,\n",
       " ('s', 'h'): 1,\n",
       " ('h', 'o'): 1,\n",
       " ('o', 'w'): 1,\n",
       " ('w', 's '): 1,\n",
       " ('e', 'v'): 1,\n",
       " ('v', 'e'): 1,\n",
       " ('r', 'a'): 1,\n",
       " ('a', 'l'): 2,\n",
       " ('l', ' '): 1,\n",
       " ('z', 'e'): 1,\n",
       " (' ', 'a'): 1,\n",
       " ('l', 'g'): 1,\n",
       " ('g', 'o'): 1,\n",
       " ('o', 'r'): 1,\n",
       " ('r', 'i'): 1,\n",
       " ('i', 'th'): 1,\n",
       " ('th', 'm'): 1,\n",
       " ('m', 's'): 1,\n",
       " ('s', '.'): 1,\n",
       " ('.', '\\n'): 1}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occs = count_occs(corpus, vocabs)\n",
    "print(max(occs.keys(), key=lambda x: occs[x]))\n",
    "occs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cbad6f8f-3e93-42b0-9cd6-1fea7195deb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'm',\n",
       " 'u',\n",
       " 'w',\n",
       " 'l',\n",
       " 'e',\n",
       " '\\n',\n",
       " 'i',\n",
       " 'v',\n",
       " 'k',\n",
       " ',',\n",
       " 'f',\n",
       " 't',\n",
       " 'h',\n",
       " ' ',\n",
       " 'n',\n",
       " '.',\n",
       " 'c',\n",
       " 'g',\n",
       " 'p',\n",
       " 'r',\n",
       " 'a',\n",
       " 'z',\n",
       " 'b',\n",
       " 's',\n",
       " 's ',\n",
       " 'th',\n",
       " 'is ',\n",
       " 'is ']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs = update_vocabs(corpus, vocabs)\n",
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7232f6-49a9-4504-aa4f-62842bc2f845",
   "metadata": {},
   "source": [
    "## Initial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f376502b-b852-462c-b465-a5021429657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "this is the hugging face course, this chapter is about tokenization. this section shows several tokenizer algorithms.\n",
    "huggingface is learning new astonishing and brilliant techniques that are superpellant. I'm happyly joining the world of ai\n",
    "as it is moving forward. I joyfolly and greatfully trying to get this done.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2d3151d4-9cd3-4029-9c02-e742a0578f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "huggingface hug face hugging hugger learning learner learners learn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86b0bd-d668-4419-a778-6d52d4a493cd",
   "metadata": {},
   "source": [
    "## Pretokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d81b00ff-e380-4b7a-973a-aac228d70dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hugger',\n",
       " 'face',\n",
       " 'learn\\n',\n",
       " 'hugging',\n",
       " 'learning',\n",
       " 'learner',\n",
       " 'learners',\n",
       " '\\nhuggingface',\n",
       " 'hug']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = corpus.replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \")\n",
    "words = list(set(corpus.split(sep=\" \")))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693b216-0da8-4db9-a2da-c0d82b89e047",
   "metadata": {},
   "source": [
    "## New Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "7faa8b0c-d5c5-499f-864e-c492aec0b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, vocabs, merges):\n",
    "    split = list(text)\n",
    "    for pair in merges:\n",
    "        i = 0\n",
    "        while i < len(split):\n",
    "            if split[i]==pair[0] and split[i+1]==pair[1]:\n",
    "                split = split[:i] + [pair[0]+pair[1]] + split[i+2:]\n",
    "            else:\n",
    "                i+=1\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "726b19f8-021f-45f8-bc49-49833278b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freqs(text, vocabs, merges):\n",
    "    itext = encode(text, vocabs, merges)\n",
    "    occs = {}\n",
    "    for token1, token2 in sliding_window(itext):\n",
    "        if (token1,token2) in occs:\n",
    "            occs[(token1,token2)]+=1\n",
    "        else:\n",
    "            occs[(token1,token2)]=1\n",
    "    return occs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "74b08ec6-1134-42e2-a411-cf48cbe1a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocabs(text, oldvocabs, oldmerges):\n",
    "    vocabs = oldvocabs.copy()\n",
    "    merges = oldmerges.copy()\n",
    "    occs = count_freqs(text, vocabs, merges)\n",
    "    keys = max(occs.keys(), key=lambda x: occs[x])\n",
    "    merges.append(keys)\n",
    "    vocabs.append(\"\".join(keys))\n",
    "    return vocabs, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "8de2c443-b365-424d-a537-98fbf40bd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = list(set(list(\" \".join(words))))\n",
    "merges = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6d1c4674-e221-4dbe-b45a-03af8832180a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"r' , 'n' , 'u' , 'a' , 'l' , 'e' , 'i' , 'f' , 'c' , 'g' , '\\n' , 's' , 'h' , ' \""
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"' , '\".join(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "4849f907-033d-41b0-8c61-4885253390a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('h', 'u')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('h', 'u'): 4,\n",
       " ('u', 'g'): 4,\n",
       " (' ', 'l'): 4,\n",
       " ('l', 'e'): 4,\n",
       " ('e', 'a'): 4,\n",
       " ('a', 'r'): 4,\n",
       " ('r', 'n'): 4,\n",
       " ('g', 'g'): 3,\n",
       " ('i', 'n'): 3,\n",
       " ('n', 'g'): 3,\n",
       " (' ', 'h'): 3,\n",
       " ('g', ' '): 3,\n",
       " ('e', 'r'): 3,\n",
       " ('g', 'i'): 2,\n",
       " ('f', 'a'): 2,\n",
       " ('a', 'c'): 2,\n",
       " ('c', 'e'): 2,\n",
       " ('e', ' '): 2,\n",
       " ('r', ' '): 2,\n",
       " ('n', 'e'): 2,\n",
       " ('\\n', 'h'): 1,\n",
       " ('g', 'f'): 1,\n",
       " (' ', 'f'): 1,\n",
       " ('g', 'e'): 1,\n",
       " ('n', 'i'): 1,\n",
       " ('r', 's'): 1,\n",
       " ('s', ' '): 1,\n",
       " ('n', '\\n'): 1}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occs = count_freqs(corpus, vocabs, merges)\n",
    "print(max(occs.keys(), key=lambda x: occs[x]))\n",
    "{key:occs[key] for key in sorted(occs.keys(), key=lambda x: occs[x], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "89abf93f-b751-42ec-ba4c-7f357d3b4f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Terminalized'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('er', ' learn')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r' , 'n' , 'u' , 'a' , 'l' , 'e' , 'i' , 'f' , 'c' , 'g' , '\n",
      "' , 's' , 'h' , ' ' , 'hu' , 'hug' , ' l' , ' le' , ' lea' , ' lear' , ' learn' , 'hugg' , 'in' , 'ing' , 'er' , 'hugging' , 'fa' , 'fac' , 'face' , 'face \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('h', 'u'),\n",
       " ('hu', 'g'),\n",
       " (' ', 'l'),\n",
       " (' l', 'e'),\n",
       " (' le', 'a'),\n",
       " (' lea', 'r'),\n",
       " (' lear', 'n'),\n",
       " ('hug', 'g'),\n",
       " ('i', 'n'),\n",
       " ('in', 'g'),\n",
       " ('e', 'r'),\n",
       " ('hugg', 'ing'),\n",
       " ('f', 'a'),\n",
       " ('fa', 'c'),\n",
       " ('fac', 'e'),\n",
       " ('face', ' ')]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "maxvocab = 100\n",
    "pattern = r\"[a-zA-Z0-9]+\\s[a-zA-Z0-9]+\"\n",
    "while len(vocabs)<maxvocab:\n",
    "    newvocabs, newmerges = update_vocabs(corpus, vocabs, merges)\n",
    "    if re.match(pattern, newmerges[-1][0]+newmerges[-1][1]):\n",
    "        display(\"Terminalized\", newmerges[-1])\n",
    "        break\n",
    "    vocabs, merges = newvocabs, newmerges\n",
    "print(\"' , '\".join(vocabs))\n",
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb3daf-ce8b-410b-a5a5-a5cc74ec8998",
   "metadata": {},
   "source": [
    "## Time to show some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b169b5a4-51c9-458a-8f65-4a25b8d5f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "9c933c86-0c95-4db1-95b1-41962f03490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[\"#362A59\", \"#3D6D46\", \"#75592B\", \"#732E31\", \"#235C73\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "19986ddd-901f-4776-a70b-10c7bb5f9c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style=\"color: white; background: #202123; font-weight: bold; padding: 20px 20px 20px 20px; font-family: 'Courier New', monospace;\">\n",
       "<span style=\"background: #362A59\">\n",
       "</span><span style=\"background: #3D6D46\">hugging</span><span style=\"background: #75592B\">face </span><span style=\"background: #732E31\">hug</span><span style=\"background: #235C73\"> </span><span style=\"background: #362A59\">face </span><span style=\"background: #3D6D46\">hugging</span><span style=\"background: #75592B\"> </span><span style=\"background: #732E31\">hugg</span><span style=\"background: #235C73\">er</span><span style=\"background: #362A59\"> learn</span><span style=\"background: #3D6D46\">ing</span><span style=\"background: #75592B\"> learn</span><span style=\"background: #732E31\">er</span><span style=\"background: #235C73\"> learn</span><span style=\"background: #362A59\">er</span><span style=\"background: #3D6D46\">s</span><span style=\"background: #75592B\"> learn</span><span style=\"background: #732E31\">\n",
       "</span>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = corpus\n",
    "tokenized = encode(text, vocabs, merges)\n",
    "mark = \"\"\n",
    "for i, token in enumerate(tokenized):\n",
    "    mark+=f'''<span style=\"background: {colors[i%len(colors)]}\">{token}</span>'''\n",
    "Markdown(f'''<div style=\"color: white; background: #202123; font-weight: bold; padding: 20px 20px 20px 20px; font-family: 'Courier New', monospace;\">\n",
    "{mark}\n",
    "</div>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735d323-22b2-4116-ac7e-520fd7436b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
